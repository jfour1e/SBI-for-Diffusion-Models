{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a0ec74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training device: cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8217a65b5f14442b0f5246788f5b80b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 10000 simulations.:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\senne\\Documents\\GitHub\\SBI-for-Diffusion-Models\\.venv\\Lib\\site-packages\\sbi\\utils\\torchutils.py:27: UserWarning: GPU was selected as a device for training the neural network. Note that we expect no significant speed ups in training for the default architectures we provide. Using the GPU will be effective only for large neural networks with operations that are fast on the GPU, e.g., for a CNN or RNN `embedding_net`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\senne\\Documents\\GitHub\\SBI-for-Diffusion-Models\\.venv\\Lib\\site-packages\\sbi\\utils\\user_input_checks.py:720: UserWarning: Data x has device 'cpu'.Moving x to the data_device 'cuda'.Training will proceed on device 'cuda:0'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\senne\\Documents\\GitHub\\SBI-for-Diffusion-Models\\.venv\\Lib\\site-packages\\sbi\\utils\\user_input_checks.py:728: UserWarning: Parameters theta has device 'cpu'. Moving theta to the data_device 'cuda'.Training will proceed on device 'cuda:0'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Neural network successfully converged after 283 epochs.Density estimator device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sbi.utils import BoxUniform\n",
    "from sbi.inference import SNPE, simulate_for_sbi\n",
    "\n",
    "from sbi_for_diffusion_models.choice_model import choice_model_simulator_torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Training device:\", device)\n",
    "\n",
    "low = torch.tensor([0, 0, -2, 0.2, 0.0], dtype=torch.float32)\n",
    "high = torch.tensor([1, 5,  2, 3.0, 1.0], dtype=torch.float32)\n",
    "\n",
    "# 1) Prior for simulation (CPU) so simulate_for_sbi samples theta on CPU.\n",
    "prior_sim = BoxUniform(low=low, high=high)\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "def simulator(th: torch.Tensor) -> torch.Tensor:\n",
    "    # Keep simulation on CPU (fastest for your current simulator).\n",
    "    # (simulate_for_sbi will pass CPU tensors because prior_sim is on CPU.)\n",
    "    return choice_model_simulator_torch(th, rng=rng, resample_invalid=True)\n",
    "\n",
    "theta, x = simulate_for_sbi(\n",
    "    simulator,\n",
    "    prior_sim,\n",
    "    num_simulations=10_000,\n",
    "    simulation_batch_size=2048,\n",
    "    num_workers=1,  # Windows-friendly\n",
    ")\n",
    "\n",
    "# 2) Prior for training (GPU). sbi asserts prior.device == training device. :contentReference[oaicite:2]{index=2}\n",
    "prior_train = BoxUniform(low=low.to(device), high=high.to(device))\n",
    "\n",
    "# 3) Train on GPU, keep data stored on CPU (recommended). :contentReference[oaicite:3]{index=3}\n",
    "inference = SNPE(prior=prior_train, device=str(device))\n",
    "density_estimator = inference.append_simulations(theta, x, data_device=\"cuda\").train(\n",
    "    training_batch_size=4096,\n",
    ")\n",
    "posterior = inference.build_posterior(density_estimator)\n",
    "\n",
    "print(\"Density estimator device:\", next(density_estimator.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fc378d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\senne\\Documents\\GitHub\\SBI-for-Diffusion-Models\\.venv\\Lib\\site-packages\\sbi\\utils\\torchutils.py:27: UserWarning: GPU was selected as a device for training the neural network. Note that we expect no significant speed ups in training for the default architectures we provide. Using the GPU will be effective only for large neural networks with operations that are fast on the GPU, e.g., for a CNN or RNN `embedding_net`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\senne\\Documents\\GitHub\\SBI-for-Diffusion-Models\\.venv\\Lib\\site-packages\\sbi\\neural_nets\\flow.py:142: UserWarning: In one-dimensional output space, this flow is limited to Gaussians\n",
      "  warn(\"In one-dimensional output space, this flow is limited to Gaussians\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Neural network successfully converged after 519 epochs."
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SNLE_A' object has no attribute 'build_likelihood'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      4\u001b[39m inference = SNLE(prior=prior_train, device=\u001b[38;5;28mstr\u001b[39m(device))\n\u001b[32m      6\u001b[39m density_estimator = (\n\u001b[32m      7\u001b[39m     inference\n\u001b[32m      8\u001b[39m     .append_simulations(theta, x, data_device=\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)   \u001b[38;5;66;03m# usually best\u001b[39;00m\n\u001b[32m      9\u001b[39m     .train(training_batch_size=\u001b[32m4096\u001b[39m)\n\u001b[32m     10\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m likelihood = \u001b[43minference\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild_likelihood\u001b[49m(density_estimator)\n",
      "\u001b[31mAttributeError\u001b[39m: 'SNLE_A' object has no attribute 'build_likelihood'"
     ]
    }
   ],
   "source": [
    "from sbi.inference import SNLE\n",
    "\n",
    "# prior_train already on CUDA\n",
    "inference = SNLE(prior=prior_train, device=str(device))\n",
    "\n",
    "density_estimator = (\n",
    "    inference\n",
    "    .append_simulations(theta, x, data_device=\"cpu\")   # usually best\n",
    "    .train(training_batch_size=4096)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fbebe3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.49954721331596375 1.0380749702453613\n"
     ]
    }
   ],
   "source": [
    "post_device = next(density_estimator.parameters()).device\n",
    "theta_d = theta.to(post_device)\n",
    "x_d = x.to(post_device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    ll = density_estimator.log_prob(x_d, context=theta_d)\n",
    "print(ll.mean().item(), ll.std().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e9f30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\senne\\Documents\\GitHub\\SBI-for-Diffusion-Models\\.venv\\Lib\\site-packages\\sbi\\neural_nets\\mnle.py:60: UserWarning: The mixed neural likelihood estimator assumes that x contains\n",
      "        continuous data in the first n-1 columns (e.g., reaction times) and\n",
      "        categorical data in the last column (e.g., corresponding choices). If\n",
      "        this is not the case for the passed `x` do not use this function.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Neural network successfully converged after 290 epochs."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\senne\\Documents\\GitHub\\SBI-for-Diffusion-Models\\.venv\\Lib\\site-packages\\sbi\\utils\\sbiutils.py:342: UserWarning: An x with a batch size of 300 was passed. It will be interpreted as a batch of independent and identically\n",
      "            distributed data X={x_1, ..., x_n}, i.e., data generated based on the\n",
      "            same underlying (unknown) parameter. The resulting posterior will be with\n",
      "            respect to entire batch, i.e,. p(theta | X).\n",
      "  warnings.warn(\n",
      "c:\\Users\\senne\\Documents\\GitHub\\SBI-for-Diffusion-Models\\.venv\\Lib\\site-packages\\sbi\\inference\\posteriors\\mcmc_posterior.py:252: UserWarning: You passed `mcmc_method` to `.sample()`. As of sbi v0.18.0, this is deprecated and will be removed in a future release. Use `method` instead of `mcmc_method`.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aed8b36e42d14208b745aeafc30d9126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running vectorized MCMC with 20 chains:   0%|          | 0/45000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sbi.inference import MNLE\n",
    "from sbi.analysis import pairplot\n",
    "from sbi.utils.get_nn_models import likelihood_nn\n",
    "\n",
    "from sbi_for_diffusion_models.rt_choice_model import (\n",
    "    rt_choice_model_simulator_torch,\n",
    "    simulate_session_data_rt_choice,\n",
    ")\n",
    "\n",
    "# ------------- 1) Define prior -------------\n",
    "# Example: adjust bounds to your problem.\n",
    "# theta = [a0_frac, lam, v, B, t_nd]\n",
    "low = torch.tensor([0.0, -2.0, 0.0, 0.2, 0.0])\n",
    "high = torch.tensor([1.0,  2.0, 5.0, 5.0, 1.0])\n",
    "\n",
    "prior = torch.distributions.Independent(\n",
    "    torch.distributions.Uniform(low, high), 1\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# ------------- 2) Generate training simulations -------------\n",
    "def simulate_training_set(num_simulations: int, batch_size: int, **sim_kwargs):\n",
    "    theta = prior.sample((num_simulations,)).to(device=device, dtype=torch.float32)\n",
    "\n",
    "    x_chunks = []\n",
    "    for start in range(0, num_simulations, batch_size):\n",
    "        th = theta[start : start + batch_size]\n",
    "        x = rt_choice_model_simulator_torch(th, **sim_kwargs)  # (B,2)\n",
    "        x_chunks.append(x.detach().cpu())\n",
    "\n",
    "    x = torch.cat(x_chunks, dim=0).to(torch.float32)  # keep stored data on CPU\n",
    "    theta_cpu = theta.detach().cpu()\n",
    "\n",
    "    assert torch.isfinite(theta_cpu).all()\n",
    "    assert torch.isfinite(x).all()\n",
    "    return theta_cpu, x\n",
    "\n",
    "\n",
    "num_simulations = 50_000\n",
    "simulation_batch_size = 2048\n",
    "\n",
    "theta_train, x_train = simulate_training_set(\n",
    "    num_simulations,\n",
    "    simulation_batch_size,\n",
    "    mu_sensory=1.0,\n",
    "    p_success=0.75,\n",
    ")\n",
    "\n",
    "# ------------- 3) Build MNLE likelihood estimator -------------\n",
    "# MNLE expects mixed data; likelihood_nn(model=\"mnle\") handles the discrete component.\n",
    "# log_transform_x=True is typically good because RTs are positive and heavy-tailed.\n",
    "estimator_builder = likelihood_nn(\n",
    "    model=\"mnle\",\n",
    "    log_transform_x=True,\n",
    "    z_score_theta=\"independent\",\n",
    "    z_score_x=\"independent\",\n",
    ")\n",
    "\n",
    "trainer = MNLE(prior=prior, density_estimator=estimator_builder)\n",
    "trainer.append_simulations(theta_train, x_train, exclude_invalid_x=False)\n",
    "trainer.train(training_batch_size=4096)\n",
    "\n",
    "# ------------- 4) Build posterior (MCMC over learned likelihood) -------------\n",
    "posterior = trainer.build_posterior(\n",
    "    prior=prior,\n",
    "    mcmc_method=\"slice_np_vectorized\",\n",
    "    mcmc_parameters=dict(\n",
    "        warmup_steps=200,\n",
    "        thin=5,\n",
    "        num_chains=20,\n",
    "        init_strategy=\"proposal\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# ------------- 5) Test on synthetic \"session\" dataset (IID trials) -------------\n",
    "theta_true = torch.tensor([0.55, 0.2, 1.5, 2.0, 0.25], dtype=torch.float32)\n",
    "x_o = simulate_session_data_rt_choice(\n",
    "    theta_true,\n",
    "    num_trials=300,\n",
    "    mu_sensory=1.0,\n",
    "    p_success=0.75,\n",
    ")\n",
    "\n",
    "# Sample posterior\n",
    "num_posterior_samples = 5_000\n",
    "samples = posterior.sample(\n",
    "    (num_posterior_samples,),\n",
    "    x=x_o,\n",
    "    mcmc_method=\"slice_np_vectorized\",\n",
    "    warmup_steps=200,\n",
    "    thin=5,\n",
    "    num_chains=20,\n",
    "    init_strategy=\"proposal\",\n",
    ")\n",
    "\n",
    "labels = [r\"$a_0$\", r\"$\\lambda$\", r\"$v$\", r\"$B$\", r\"$t_{nd}$\"]\n",
    "fig, ax = pairplot(\n",
    "    [prior.sample((2000,)), samples],\n",
    "    points=theta_true.unsqueeze(0),\n",
    "    diag=\"kde\",\n",
    "    upper=\"kde\",\n",
    "    labels=labels,\n",
    ")\n",
    "plt.suptitle(\"MNLE posterior vs prior (RT+choice)\", fontsize=14)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbi-for-diffusion-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
