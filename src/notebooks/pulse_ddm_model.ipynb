{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d75fa6f2",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59029e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.distributions import Beta, Uniform, LogNormal, HalfNormal, TransformedDistribution, ExpTransform\n",
    "\n",
    "import sbi\n",
    "from sbi.utils import BoxUniform, MultipleIndependent\n",
    "from sbi.inference import MNLE\n",
    "import sbi.neural_nets as nn\n",
    "from sbi.inference.posteriors.mcmc_posterior import MCMCPosterior\n",
    "from sbi.analysis import sbc_rank_plot, pairplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd18484",
   "metadata": {},
   "source": [
    "#### Define DDM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c1d20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= pulse-DDM model parameters =========\n",
    "\n",
    "DT = 0.005   # time step\n",
    "T_MAX = 8.0  # total trial window\n",
    "T0 = 0.1     # fixed non-decision time\n",
    "T_DEC = T_MAX - T0 # diffusion process window\n",
    "N_STEPS = int(T_DEC / DT)\n",
    "\n",
    "PULSE_INTERVAL = 0.1\n",
    "STEPS_PER_PULSE = int(PULSE_INTERVAL / DT)\n",
    "N_PULSES = N_STEPS // STEPS_PER_PULSE + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a24655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_pulse_ddm_single(theta: np.ndarray, rng: np.random.Generator,) -> tuple[float, int]:\n",
    "    \"\"\"\n",
    "    Simulate ONE trial from the pulse-DDM.\n",
    "    theta = [a0, lam, nu, B, sigma]\n",
    "\n",
    "    inputs:\n",
    "        theta: numpy array of the form: [a0, lam, nu, B, sigma]\n",
    "        rng: random number object\n",
    "\n",
    "    Returns:\n",
    "        rt: reaction time in seconds (float)\n",
    "          choice_idx = 0 (left/lower bound), 1 (right/upper bound)\n",
    "    \"\"\"\n",
    "    a0, lam, nu, B, sigma = theta.astype(np.float32)\n",
    "    # keep these params positive\n",
    "    B = abs(B)\n",
    "    lam = abs(lam)\n",
    "    nu = abs(nu)\n",
    "    sigma = abs(sigma)\n",
    "\n",
    "    # starting point = a0 * B\n",
    "    a0 = np.clip(a0, 0.0, 1.0)\n",
    "    a_curr = float(np.clip(a0, 0.0, 1.0) * B)\n",
    "\n",
    "    # Sample pulse directions randomly: (-1 or 1)\n",
    "    s_seq = np.where(rng.random(size=N_PULSES) < 0.5, 1.0, -1.0).astype(np.float32)\n",
    "\n",
    "    # Precompute information jump schedule over time steps\n",
    "    pulse_indices = np.arange(0, N_STEPS, STEPS_PER_PULSE, dtype=int)\n",
    "    jump_t = np.zeros(N_STEPS, dtype=np.float32)\n",
    "    s_for_schedule = s_seq[: pulse_indices.shape[0]]\n",
    "    jump_t[pulse_indices] = s_for_schedule * nu\n",
    "\n",
    "    done = False\n",
    "    first_hit_time = -1.0\n",
    "    choice_idx = None\n",
    "\n",
    "    # Euler–Maruyama integration of diffusion SDE\n",
    "    for t in range(N_STEPS):\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        drift = -lam * a_curr * DT\n",
    "        jump = jump_t[t]\n",
    "        noise = sigma * np.sqrt(DT) * rng.normal()\n",
    "\n",
    "        a_next = a_curr + drift + jump + noise\n",
    "\n",
    "        # Hitting bounds: lower = 0, upper = B\n",
    "        if a_next >= B:\n",
    "            done = True\n",
    "            choice_idx = 1  # upper\n",
    "            first_hit_time = (t + 1) * DT\n",
    "\n",
    "        elif a_next <= 0.0:\n",
    "            done = True\n",
    "            choice_idx = 0  # lower\n",
    "            first_hit_time = (t + 1) * DT\n",
    "\n",
    "        a_curr = a_next\n",
    "\n",
    "    if not done:\n",
    "        first_hit_time = T_DEC\n",
    "        choice_idx = 1 if a_curr >= (B / 2.0) else 0\n",
    "\n",
    "    rt = float(np.clip(T0 + first_hit_time, 1e-3, T_MAX))\n",
    "\n",
    "    return rt, int(choice_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf89781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pulse_ddm_simulator_torch(theta: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        theta: torch.Tensor of parameters: [lam, nu, B, sigma]\n",
    "\n",
    "    Returns:\n",
    "        x: torch.Tensor with columns: [rt, choice]\n",
    "    \"\"\"\n",
    "    theta_np = theta.detach().cpu().numpy().astype(np.float32)\n",
    "    batch_size = theta_np.shape[0]\n",
    "    xs = np.zeros((batch_size, 2), dtype=np.float32)\n",
    "\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        rt, choice_idx = simulate_pulse_ddm_single(theta_np[i], rng)\n",
    "        xs[i, 0] = rt\n",
    "        xs[i, 1] = choice_idx\n",
    "\n",
    "    return torch.from_numpy(xs).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd17631",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37e0213d",
   "metadata": {},
   "source": [
    "#### Build NN for MNLE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a75750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Define Priors over parameters (provide range for MCMC)=========\n",
    "\n",
    "\"\"\"\n",
    "paramter a0:\n",
    "\n",
    "Introduce the bias parameter a0: Beta(2,2), weighted towards 0.5,\n",
    "parameter nu: log(nu) ~ normal(log(1), 0.5)\n",
    "paramter B: normal(log(2), 0.5)\n",
    "paramter sigma: half normal, or exponential -- first try half normal (to keep positive)\n",
    "\"\"\"\n",
    "prior = MultipleIndependent(\n",
    "    [\n",
    "        # a0 ~ Beta(2,2)\n",
    "        Beta(\n",
    "            concentration1=torch.tensor([2.0], dtype=torch.float32),\n",
    "            concentration0=torch.tensor([2.0], dtype=torch.float32),\n",
    "        ),\n",
    "\n",
    "        # lam ~ HalfNormal(0.5)\n",
    "        HalfNormal(\n",
    "            scale=torch.tensor([0.5], dtype=torch.float32),\n",
    "        ),\n",
    "\n",
    "        # nu ~ LogNormal(log 1, 0.5)\n",
    "        LogNormal(\n",
    "            loc=torch.tensor([np.log(1.0)], dtype=torch.float32),\n",
    "            scale=torch.tensor([0.5], dtype=torch.float32),\n",
    "        ),\n",
    "\n",
    "        # B ~ LogNormal(log 2, 0.5)\n",
    "        LogNormal(\n",
    "            loc=torch.tensor([np.log(2.0)], dtype=torch.float32),\n",
    "            scale=torch.tensor([0.5], dtype=torch.float32),\n",
    "        ),\n",
    "\n",
    "        # sigma ~ HalfNormal(0.5)\n",
    "        HalfNormal(\n",
    "            scale=torch.tensor([0.5], dtype=torch.float32),\n",
    "        ),\n",
    "    ],\n",
    "    validate_args=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d205f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mnle(\n",
    "    num_simulations: int = 10_000,\n",
    "    simulation_batch_size: int = 512,\n",
    "    mcmc_kwargs: dict | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train MNLE for the pulse-DDM and return an MCMC-based posterior object.\n",
    "\n",
    "    The learned single-trial likelihood is:\n",
    "        x = (rt, correctness_idx), correctness_idx ∈ {0,1}.\n",
    "    \"\"\"\n",
    "\n",
    "    if mcmc_kwargs is None:\n",
    "        mcmc_kwargs = dict(\n",
    "            num_chains=50,\n",
    "            warmup_steps=200,\n",
    "            thin=5,\n",
    "            init_strategy=\"proposal\",\n",
    "        )\n",
    "\n",
    "    # Sample parameters from the prior -- specified by Ryan\n",
    "    theta_train = prior.sample((num_simulations,)).to(torch.float32)\n",
    "    x_train_list = []\n",
    "\n",
    "    # generate single trial training data with the simulator\n",
    "    for start in range(0, num_simulations, simulation_batch_size):\n",
    "        batch = theta_train[start:start+simulation_batch_size]\n",
    "        x_batch = pulse_ddm_simulator_torch(batch)\n",
    "        x_train_list.append(x_batch)\n",
    "\n",
    "    x_train = torch.cat(x_train_list, dim=0).to(torch.float32)\n",
    "\n",
    "    # nan logic\n",
    "    assert torch.isfinite(theta_train).all(), \"NaN/Inf in theta_train.\"\n",
    "    assert torch.isfinite(x_train).all(), \"NaN/Inf in x_train.\"\n",
    "\n",
    "    # Define the MNLE likelihood network builder\n",
    "    estimator_builder = likelihood_nn(\n",
    "        model=\"mnle\",\n",
    "        x_num_conDim=1,\n",
    "        log_transform_x=True,\n",
    "        z_score_theta=\"independent\",\n",
    "        z_score_x=\"independent\",\n",
    "    )\n",
    "\n",
    "    # Initialize MNLE trainer, train on (theta, x) pairs\n",
    "    trainer = MNLE(prior, estimator_builder)\n",
    "    _ = trainer.append_simulations(\n",
    "        theta_train,\n",
    "        x_train,\n",
    "        exclude_invalid_x=False,\n",
    "    ).train()\n",
    "\n",
    "    # build MCMC-based posterior from the trained likelihood estimator\n",
    "    posterior = trainer.build_posterior(\n",
    "        prior=prior,\n",
    "        posterior_parameters=MCMCPosteriorParameters(\n",
    "            method=\"slice_np_vectorized\",\n",
    "            **mcmc_kwargs,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return posterior, trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae10bad7",
   "metadata": {},
   "source": [
    "#### Parameter recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7296e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_session_data(theta_true: Tensor, num_trials: int = 1000) -> Tensor:\n",
    "    theta_np = theta_true.detach().cpu().numpy().astype(np.float32)\n",
    "    xs = np.zeros((num_trials, 2), dtype=np.float32)\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    for n in range(num_trials):\n",
    "        rt, choice_idx = simulate_pulse_ddm_single(theta_np, rng)\n",
    "        xs[n, 0] = rt\n",
    "        xs[n, 1] = choice_idx\n",
    "\n",
    "    return torch.from_numpy(xs).to(torch.float32)\n",
    "\n",
    "def run_parameter_recovery(\n",
    "    num_simulations=50_000,\n",
    "    num_trials_session=200,\n",
    "    num_posterior_samples=5_000,\n",
    "    theta_true=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train MNLE once, then test how well it can recover θ_true from a synthetic dataset\n",
    "    \"\"\"\n",
    "    posterior, trainer = train_mnle(num_simulations)\n",
    "\n",
    "    if theta_true is None:\n",
    "        theta_true = torch.tensor([0.5, 0.4, 1.0, 2.0, 0.2], dtype=torch.float32)\n",
    "\n",
    "    theta_true = theta_true.to(torch.float32)\n",
    "\n",
    "    x_o = simulate_session_data(theta_true, num_trials_session)\n",
    "\n",
    "    posterior_samples = posterior.sample(\n",
    "        sample_shape=(num_posterior_samples,),\n",
    "        x=x_o,\n",
    "    )\n",
    "\n",
    "    labels = [r\"$a_0$\", r\"$\\lambda$\", r\"$\\nu$\", r\"$B$\", r\"$\\sigma$\"]\n",
    "\n",
    "    fig, ax = pairplot(\n",
    "        [prior.sample((2000,)), posterior_samples],\n",
    "        points=theta_true.unsqueeze(0),\n",
    "        diag=\"kde\",\n",
    "        upper=\"kde\",\n",
    "        labels=labels,\n",
    "    )\n",
    "    plt.suptitle(\"Parameter Recovery: MNLE posterior vs prior\", fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "    return theta_true, x_o, posterior, posterior_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f569e6f",
   "metadata": {},
   "source": [
    "#### Posterior Predictive Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482dc01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_predictive_checks(\n",
    "    posterior,\n",
    "    x_o: Tensor,\n",
    "    num_sessions: int = 50,\n",
    "    num_trials_per_session: int | None = None,\n",
    "    num_posterior_samples: int = 1_000,\n",
    "):\n",
    "    \"\"\"\n",
    "    Post Hoc Analysis: Compare RT and choice statistics between observed and posterior predictive datasets.\n",
    "\n",
    "    Inputs:\n",
    "        posterior: sbi posterior (MCMC Posterior) from MNLE fitting\n",
    "        x_o: observed data, shape (N_trials, 2) [rt, choice_idx]\n",
    "        num_sessions: number of posterior predictive datasets to draw\n",
    "        num_trials_per_session: optional; if None, use len(x_o)\n",
    "    \"\"\"\n",
    "    if num_trials_per_session is None:\n",
    "        num_trials_per_session = x_o.shape[0]\n",
    "\n",
    "    # ---------- 1) Sample from posterior ONCE ----------\n",
    "    with torch.no_grad():\n",
    "        theta_samples = posterior.sample(\n",
    "            sample_shape=(num_posterior_samples,),\n",
    "            x=x_o,\n",
    "        )  # shape: [S, 4]\n",
    "\n",
    "    theta_samples_np = theta_samples.cpu().numpy()\n",
    "    S = theta_samples_np.shape[0]\n",
    "\n",
    "    # ---------- 2) Compute observed summary stats ----------\n",
    "    x_o_np = x_o.cpu().numpy()\n",
    "    rt_obs = x_o_np[:, 0]\n",
    "    choice_idx_obs = x_o_np[:, 1]  # 0 = left, 1 = right (internal coding)\n",
    "\n",
    "    obs_mean_rt = float(rt_obs.mean())\n",
    "    obs_p_right = float(choice_idx_obs.mean())  # since this is 0/1, mean = P(right)\n",
    "\n",
    "    # ---------- 3) Simulate posterior-predictive datasets ----------\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    rep_mean_rt = []\n",
    "    rep_p_right = []\n",
    "\n",
    "    for i in range(num_sessions):\n",
    "        # pick one θ sample from posterior\n",
    "        idx = rng.integers(0, S)\n",
    "        theta_i = torch.from_numpy(theta_samples_np[idx])\n",
    "\n",
    "        # simulate a session at that θ\n",
    "        x_rep = simulate_session_data(theta_i, num_trials=num_trials_per_session)\n",
    "        x_rep_np = x_rep.numpy()\n",
    "\n",
    "        rt_rep = x_rep_np[:, 0]\n",
    "        choice_idx_rep = x_rep_np[:, 1]\n",
    "\n",
    "        rep_mean_rt.append(rt_rep.mean())\n",
    "        rep_p_right.append(choice_idx_rep.mean())\n",
    "\n",
    "    rep_mean_rt = np.array(rep_mean_rt)\n",
    "    rep_p_right = np.array(rep_p_right)\n",
    "\n",
    "    # ---------- 4) Plot PPC summary histograms ----------\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n",
    "\n",
    "    # Mean RT across sessions\n",
    "    axes[0].hist(rep_mean_rt, bins=20, alpha=0.8)\n",
    "    axes[0].axvline(obs_mean_rt, color=\"red\", linestyle=\"--\", label=\"observed\")\n",
    "    axes[0].set_xlabel(\"Mean RT (s)\")\n",
    "    axes[0].set_ylabel(\"Count\")\n",
    "    axes[0].set_title(\"PPC: session mean RT\")\n",
    "    axes[0].legend(frameon=False)\n",
    "\n",
    "    # Proportion right across sessions\n",
    "    axes[1].hist(rep_p_right, bins=20, alpha=0.8)\n",
    "    axes[1].axvline(obs_p_right, color=\"red\", linestyle=\"--\", label=\"observed\")\n",
    "    axes[1].set_xlabel(\"P(right)\")\n",
    "    axes[1].set_ylabel(\"Count\")\n",
    "    axes[1].set_title(\"PPC: session P(right)\")\n",
    "    axes[1].set_xlim(0, 1)\n",
    "    axes[1].legend(frameon=False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f90de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Simulation-Based Calibration (SBC) =========\n",
    "\n",
    "def simulate_dataset_for_sbc(theta_batch: Tensor, num_trials: int) -> list[Tensor]:\n",
    "    \"\"\"\n",
    "    For each θ in theta_batch, simulate a dataset of num_trials trials.\n",
    "    Returns list of tensors, each of shape (num_trials, 2).\n",
    "    \"\"\"\n",
    "    datasets = []\n",
    "    for i in range(theta_batch.shape[0]):\n",
    "        x_i = simulate_session_data(theta_batch[i], num_trials=num_trials)\n",
    "        datasets.append(x_i)\n",
    "    return datasets\n",
    "\n",
    "from sbi.diagnostics import run_sbc\n",
    "from sbi.analysis.plot import sbc_rank_plot\n",
    "\n",
    "def run_sbc_for_mnle(\n",
    "    posterior,\n",
    "    num_sbc_samples: int = 20,\n",
    "    num_trials_per_dataset: int = 20,\n",
    "    num_posterior_samples: int = 200,\n",
    "):\n",
    "    thetas = prior.sample((num_sbc_samples,))\n",
    "\n",
    "    # ---------- 2) Simulate one iid dataset x_i for each theta_i ----------\n",
    "    # We'll store them as a 3D tensor: (num_sbc_samples, num_trials, 2)\n",
    "    xs_list = []\n",
    "    for i in range(num_sbc_samples):\n",
    "        theta_i = thetas[i]                # shape: (4,)\n",
    "        x_i = simulate_session_data(       # uses your NumPy-based simulator\n",
    "            theta_i,\n",
    "            num_trials=num_trials_per_dataset,\n",
    "        )                                  # shape: (num_trials, 2)\n",
    "        xs_list.append(x_i)\n",
    "\n",
    "    xs = torch.stack(xs_list, dim=0)       # (num_sbc_samples, num_trials, 2)\n",
    "\n",
    "    # ---------- 3) Run SBC ----------\n",
    "    # IMPORTANT: pass thetas & xs POSITIONALLY, not by the old keyword names.\n",
    "    ranks, dap_samples = run_sbc(\n",
    "        thetas,\n",
    "        xs,\n",
    "        posterior,\n",
    "        num_posterior_samples=num_posterior_samples,\n",
    "        use_batched_sampling=False,   # safer on memory for MNLE+MCMC posteriors\n",
    "        num_workers=1,                # bump if you want CPU parallelism\n",
    "    )\n",
    "\n",
    "    # ---------- 4) Plot SBC rank histograms ----------\n",
    "    fig, ax = sbc_rank_plot(\n",
    "        ranks,\n",
    "        num_posterior_samples,\n",
    "        num_bins=20,\n",
    "        figsize=(5, 3),\n",
    "    )\n",
    "\n",
    "    return ranks, dap_samples, fig, ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23752e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_empirical_rt_choice(x_o: torch.Tensor, title: str = \"Observed data\"):\n",
    "    \"\"\"\n",
    "    Quick visualization of RT distribution and choice proportion for a dataset x_o.\n",
    "    x_o: (N_trials, 2) with columns [rt, choice_idx_mnle] (0=left, 1=right internally)\n",
    "    \"\"\"\n",
    "    x_np = x_o.numpy()\n",
    "    rt = x_np[:, 0]\n",
    "    choice_idx = x_np[:, 1]\n",
    "    p_right = choice_idx.mean()\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n",
    "\n",
    "    axes[0].hist(rt, bins=30, alpha=0.8)\n",
    "    axes[0].set_xlabel(\"RT (s)\")\n",
    "    axes[0].set_ylabel(\"Count\")\n",
    "    axes[0].set_title(f\"{title}: RT distribution\")\n",
    "\n",
    "    axes[1].bar([\"Left\", \"Right\"], [1.0 - p_right, p_right])\n",
    "    axes[1].set_ylim(0, 1)\n",
    "    axes[1].set_title(f\"{title}: choice proportions\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_posterior_marginals(posterior_samples: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Simple marginal histograms for each parameter from posterior samples.\n",
    "    \"\"\"\n",
    "    labels = [r\"$\\lambda$\", r\"$\\nu$\", r\"$B$\", r\"$\\sigma$\", r\"a0\"]\n",
    "    samples_np = posterior_samples.numpy()\n",
    "    fig, axes = plt.subplots(1, samples_np.shape[1], figsize=(12, 3))\n",
    "    for i in range(samples_np.shape[1]):\n",
    "        axes[i].hist(samples_np[:, i], bins=30, alpha=0.8)\n",
    "        axes[i].set_title(labels[i])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab7075b",
   "metadata": {},
   "source": [
    "#### MNLE pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63246f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">>> Training MNLE + parameter recovery...\")\n",
    "posterior, trainer = train_mnle(num_simulations=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076e47cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run parameter recovery\n",
    "theta_true, x_o, posterior, posterior_samples = run_parameter_recovery(\n",
    "    num_simulations=10_000,\n",
    "    num_trials_session=20,\n",
    "    num_posterior_samples=2_000,\n",
    ")\n",
    "\n",
    "# Plot empirical data summaries for the observed synthetic session\n",
    "plot_empirical_rt_choice(x_o, title=\"Synthetic session (θ_true)\")\n",
    "\n",
    "# Plot posterior marginals for θ\n",
    "plot_posterior_marginals(posterior_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925b0d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_predictive_checks(\n",
    "    posterior,\n",
    "    x_o,\n",
    "    num_sessions=50,              # 50 fake sessions\n",
    "    num_trials_per_session=x_o.shape[0],\n",
    "    num_posterior_samples=1_000,  # 1k θ samples for reuse\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1445b7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks, dap_samples, fig_sbc, ax_sbc = run_sbc_for_mnle(\n",
    "    posterior,\n",
    "    num_sbc_samples=5,\n",
    "    num_trials_per_dataset=20,\n",
    "    num_posterior_samples=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d62b9a",
   "metadata": {},
   "source": [
    "#### Run code on rat data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82e77d9",
   "metadata": {},
   "source": [
    "Inside the generate_session_data() function, for Real data, the following must be changed:\n",
    "- correct_side replaced from the task,\n",
    "- logic for correctness_idx is same: is_correct = (choice == correct_side) and correctness_idx logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8ff609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframe:\n",
    "\n",
    "rat_df = pd.read_csv(\"rat_data_clean.csv\")\n",
    "\n",
    "# we must fit the model per rat:\n",
    "# print(rat_df[\"name\"].unique())\n",
    "\n",
    "rat_individual_df = rat_df[rat_df[\"name\"] == 1054].reset_index()\n",
    "\n",
    "rat_individual_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633a176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_x_from_rat_df(rat_individual_df) -> torch.Tensor:\n",
    "    df = rat_individual_df.copy()\n",
    "    df = df.dropna(subset=[\"rt\", \"outcome\"])\n",
    "\n",
    "    rts = df[\"rt\"].to_numpy(dtype=\"float32\")\n",
    "    correctness = df[\"outcome\"].to_numpy(dtype=\"float32\")  # 0/1\n",
    "\n",
    "    rt_tensor = torch.from_numpy(rts)\n",
    "    correctness_tensor = torch.from_numpy(correctness)\n",
    "\n",
    "    x = torch.stack([rt_tensor, correctness_tensor], dim=1)  # (num_trials, 2)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bd1017",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use posterior and trainer from train_mnle\n",
    "\n",
    "Select one individual: rat_individual_df, preprocess and get posterior samples.\n",
    "\n",
    "\"\"\"\n",
    "posterior, trainer = train_mnle(num_simulations=50_000)\n",
    "\n",
    "x_rat = make_x_from_rat_df(rat_individual_df)\n",
    "\n",
    "num_posterior_samples = 5_000\n",
    "\n",
    "posterior_samples_rat = posterior.sample(\n",
    "    sample_shape=(num_posterior_samples,),\n",
    "    x=x_rat,\n",
    ")\n",
    "labels = [r\"$\\lambda$\", r\"$\\nu$\", r\"$B$\", r\"$\\sigma$\"]\n",
    "\n",
    "fig, ax = pairplot(\n",
    "    [prior.sample((2_000,)), posterior_samples_rat],\n",
    "    diag=\"kde\",\n",
    "    upper=\"kde\",\n",
    "    labels=labels,\n",
    ")\n",
    "plt.suptitle(\"Rat 1054: posterior vs prior\", fontsize=14)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SBI-for-Diffusion-Models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
